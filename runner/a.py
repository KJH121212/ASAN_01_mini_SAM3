# ... (ì•ë¶€ë¶„ import ë° ì„¤ì • ì½”ë“œ ë™ì¼) ...

# ==========================================
# 2. 10ê°œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ (0 ~ 9)
# ==========================================
TARGET_INDICES = range(10)

for target in TARGET_INDICES:
    print(f"\n{'='*50}")
    print(f"ğŸ¬ Target {target} ì²˜ë¦¬ ì‹œì‘")
    print(f"{'='*50}")

    # --- [A. ê²½ë¡œ ì„¤ì • ë° BBox ë¡œë“œ] ---
    # (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    COMMON_PATH = df.loc[target, "common_path"]
    VIDEO_PTH = Path(df.loc[target, "video_path"])
    
    KPT_DIR = DATA_DIR / "2_KEYPOINTS" / COMMON_PATH
    JSON_PATH = KPT_DIR / "000000.json" 

    SAVE_ROOT = DATA_DIR / "test" / COMMON_PATH
    SAVE_ROOT.mkdir(parents=True, exist_ok=True)
    
    OUTPUT_JSON = SAVE_ROOT / "tracking_results.json"
    OUTPUT_MP4 = SAVE_ROOT / f"{COMMON_PATH}_result.mp4"
    TIME_LOG = SAVE_ROOT / "time_log.txt"
    
    # --------------------------------------
    # B. ë¹„ë””ì˜¤ ì •ë³´ ì½ê¸° ë° BBox ì •ê·œí™”
    # --------------------------------------
    cap = cv2.VideoCapture(str(VIDEO_PTH))
    if not cap.isOpened():
        print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VIDEO_PTH}")
        continue

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    TGT_W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    TGT_H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    # BBox ë¡œë“œ ë° ì •ê·œí™” (SAM ì…ë ¥ìš© 0.0~1.0 ë¹„ìœ¨)
    norm_box_input = None
    if JSON_PATH.exists():
        with open(JSON_PATH, 'r') as f:
            data = json.load(f)
        
        raw_bbox = data["instance_info"][0]["bbox"][0]
        # [í•µì‹¬] ì •ê·œí™” (0.0~1.0 ë¹„ìœ¨ë¡œ ë³€í™˜)
        norm_box = [
            raw_bbox[0] / SRC_W,
            raw_bbox[1] / SRC_H,
            raw_bbox[2] / SRC_W,
            raw_bbox[3] / SRC_H
        ]
        # SAM ì…ë ¥ìš© TENSOR í˜•íƒœ (Batch ì°¨ì› ì¶”ê°€)
        norm_box_input = torch.tensor(norm_box, dtype=torch.float32).unsqueeze(0).to(device)
        
        print(f"ğŸ“¦ BBox ì •ê·œí™” ì™„ë£Œ (0~1): {norm_box}")
    else:
        print(f"âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {JSON_PATH}")
        cap.release()
        continue 
    
    # --------------------------------------
    # C. ëª¨ë¸ ì´ˆê¸°í™” ë° í”„ë¡¬í”„íŠ¸ ì£¼ì… (Chunking ë£¨í”„ ì œê±°)
    # --------------------------------------
    start_time = time.time()
    
    # 1. ëª¨ë¸ ìƒíƒœ ì´ˆê¸°í™” (ë¹„ë””ì˜¤ ì „ì²´ ë¡œë“œ, CPU Offload ì‚¬ìš©)
    predictor.image_size = 1024 # ì•ˆì „í•œ 1024 í•´ìƒë„ ë³µêµ¬
    inference_state = predictor.init_state(
        video_path=str(VIDEO_PTH),
        offload_video_to_cpu=True,   # ê¸´ ì˜ìƒì— í•„ìˆ˜
        offload_state_to_cpu=True,   # ê¸´ ì˜ìƒì— í•„ìˆ˜
        async_loading_frames=True
    )

    # 2. [í•µì‹¬] ì²« í”„ë ˆì„ì— BBox í”„ë¡¬í”„íŠ¸ ì£¼ì…
    # ì´ ê³¼ì •ì€ ì „ì²´ ì¶”ì  ì „ì— ë‹¨ í•œ ë²ˆë§Œ ìˆ˜í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    _, _, _, _ = predictor.add_new_points_or_box(
        inference_state=inference_state,
        frame_idx=0,
        obj_id=1, # ê°ì²´ ID 1
        box=norm_box_input, 
        points=None,
        labels=None
    )
    print("âœ… ì´ˆê¸° í”„ë¡¬í”„íŠ¸(BBox) ì£¼ì… ì™„ë£Œ.")

    # --------------------------------------
    # D. ì¶”ì  ë° ì €ì¥ ë£¨í”„
    # --------------------------------------
    json_results = {}
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out_writer = cv2.VideoWriter(str(OUTPUT_MP4), fourcc, fps, (TGT_W, TGT_H))
    
    # 1. ì¶”ì  ì‹œì‘ (í”„ë ˆì„ 0ë¶€í„° ëê¹Œì§€)
    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(
        inference_state,
        start_frame_idx=0, # 0ë²ˆë¶€í„° ì‹œì‘
        reverse=False,
        propagate_preflight=True
    ):
        
        # 2. JSON ë°ì´í„° ìˆ˜ì§‘
        json_results[out_frame_idx] = {
            "obj_ids": [int(id) for id in out_obj_ids],
            # ë§ˆìŠ¤í¬ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ë“±ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        }

        # 3. ì˜ìƒ ì²˜ë¦¬ ë° ì €ì¥
        cap.set(cv2.CAP_PROP_POS_FRAMES, out_frame_idx) # cap ìœ„ì¹˜ë¥¼ í˜„ì¬ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
        ret, frame = cap.read()
        
        if not ret: 
            print(f"âš ï¸ í”„ë ˆì„ {out_frame_idx}ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨.")
            break
        
        if len(out_mask_logits) > 0:
            mask = (out_mask_logits[0] > 0.0).cpu().numpy().astype(np.uint8).squeeze()
            
            # ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ (SAM ì¶œë ¥ì„ ì›ë³¸ í•´ìƒë„ì— ë§ì¶¤)
            if mask.shape[:2] != (TGT_H, TGT_W):
                mask = cv2.resize(mask, (TGT_W, TGT_H), interpolation=cv2.INTER_NEAREST)
            
            # ë§ˆìŠ¤í¬ í•©ì„± (ë…¹ìƒ‰ ì˜¤ë²„ë ˆì´)
            colored_mask = np.zeros_like(frame)
            colored_mask[mask == 1] = MASK_COLOR
            frame = cv2.addWeighted(frame, 1.0, colored_mask, 0.5, 0)

        out_writer.write(frame)
    
    # --------------------------------------
    # E. ë§ˆë¬´ë¦¬ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
    # --------------------------------------
    cap.release()
    out_writer.release()
    
    with open(OUTPUT_JSON, 'w') as f:
        json.dump(json_results, f, indent=4)

    elapsed_time = time.time() - start_time
    
    with open(TIME_LOG, 'w') as f:
        f.write(f"Target: {target}\nProcessing Time: {elapsed_time:.2f}s\n")

    print(f"âœ… Target {target} ì™„ë£Œ! (ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    del inference_state
    gc.collect()
    torch.cuda.empty_cache()

print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì¢…ë£Œ!")